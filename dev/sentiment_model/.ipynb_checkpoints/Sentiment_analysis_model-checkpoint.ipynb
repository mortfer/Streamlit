{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ca36f64",
   "metadata": {},
   "source": [
    "# Training of sentiment analysis model with the embeddings computed on previous notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a374e3d8",
   "metadata": {},
   "source": [
    "41k spanish tweets about football, 15k spanish amazon reviews, ~4k spansih movie reviews, 5k english movie reviews and ~7k spanish UOC's tweets about politics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2abdf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', 250)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fe47ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_path='../../data/sentiment_datasets_embeddings/'\n",
    "\n",
    "#EN_MOVIES\n",
    "with open(embeddings_path+'IMDB_Dataset_embeddings.npy', 'rb') as f:\n",
    "     IMDB_Dataset_embeddings_tmp=np.load(f)\n",
    "with open(embeddings_path+'IMDB_Dataset_labels.npy', 'rb') as f:\n",
    "     IMDB_Dataset_labels_tmp=np.load(f)\n",
    "        \n",
    "#ES_MOVIES  \n",
    "with open(embeddings_path+'es_movies_embeddings.npy', 'rb') as f:\n",
    "     es_movies_embeddings=np.load(f)\n",
    "with open(embeddings_path+'es_movies_labels.npy', 'rb') as f:\n",
    "     es_movies_labels=np.load(f)\n",
    "        \n",
    "#ES_REVIEWS\n",
    "with open(embeddings_path+'es_reviews_embeddings.npy', 'rb') as f:\n",
    "     es_reviews_embeddings=np.load(f)\n",
    "with open(embeddings_path+'es_reviews_labels.npy', 'rb') as f:\n",
    "     es_reviews_labels=np.load(f)\n",
    "        \n",
    "#ES_TWEETS\n",
    "with open(embeddings_path+'spanish_tweets_sentiment_embeddings.npy', 'rb') as f:\n",
    "     spanish_tweets_sentiment_embeddings=np.load(f)\n",
    "with open(embeddings_path+'spanish_tweets_sentiment_labels.npy', 'rb') as f:\n",
    "     spanish_tweets_sentiment_labels=np.load(f)\n",
    "        \n",
    "#UOC_TWEETS\n",
    "with open(embeddings_path+'UOC_tweets_embeddings.npy', 'rb') as f:\n",
    "     UOC_tweets__embeddings=np.load(f)\n",
    "with open(embeddings_path+'UOC_tweets_labels.npy', 'rb') as f:\n",
    "     UOC_tweets_labels=np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ab6243b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3872, 768)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_movies_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "095b8f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#En el dataset de reviews en inglés hay más de las que quiero así que elijo 5k al azar (balanceado)\n",
    "np.random.seed(6)\n",
    "indices=np.random.randint(0,len(IMDB_Dataset_labels_tmp),5000)\n",
    "pd.Series(IMDB_Dataset_labels_tmp[indices]).value_counts() #2.5k negativas  y 2.5k positivas\n",
    "IMDB_Dataset_embeddings=IMDB_Dataset_embeddings_tmp[indices,:]\n",
    "IMDB_Dataset_labels=IMDB_Dataset_labels_tmp[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "359e9f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((73191, 768), (73191,))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Junto todo\n",
    "embeddings=np.concatenate((IMDB_Dataset_embeddings,es_movies_embeddings,es_reviews_embeddings,\n",
    "                           spanish_tweets_sentiment_embeddings,UOC_tweets__embeddings))\n",
    "labels=np.concatenate((IMDB_Dataset_labels,es_movies_labels,es_reviews_labels,\n",
    "                       spanish_tweets_sentiment_labels,UOC_tweets_labels))\n",
    "embeddings.shape,labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "197337ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Separo datos en 80% entreno 20% test\n",
    "x_train,x_test,y_train,y_test = train_test_split(embeddings,labels,stratify=labels,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d2170451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0    20460\n",
       " 1    19880\n",
       "-1    18212\n",
       "dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_train).value_counts() #Esta ligeramente desbalanceado, no mucho, así que sigo con esto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0520cafd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.1: 0.7555160871644238\n",
      "Accuracy for C=1: 0.7615957374137577\n",
      "Accuracy for C=10: 0.7624154655372635\n",
      "Wall time: 7min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Cuanto mayor es C, más tarda en entrenar\n",
    "for c in [0.1,1,10]:    \n",
    "    sentiment_model = LogisticRegression(C=c,max_iter=1500,n_jobs=-1)\n",
    "    sentiment_model.fit(x_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_test, sentiment_model.predict(x_test)))) \n",
    "#Me quedo con C=1 ya que da buenos resultados y con C=10 puede haber más overfitting.\n",
    "#Aunque y_test nunca lo ha visto el modelo, sí que son datos provenientes del mismo dataset y aunque haya intentando variar,\n",
    "#no son represetnativos de todos los tweets que usaré. Además, la mejora de precisión es despreciable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7e2afc25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7542181843022064\n"
     ]
    }
   ],
   "source": [
    "#Pruebo random forest a ver si hay mejora sustancial\n",
    "rf = RandomForestClassifier(n_jobs=-1)\n",
    "rf.fit(x_train, y_train)\n",
    "print(accuracy_score(y_test, rf.predict(x_test)))\n",
    "#No la mejora no es significativa así que no me preocupo en buscar hyperparámetros. Me quedo con regriós logística que es simple\n",
    "#y efectiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f43696f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 25s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, max_iter=1500, n_jobs=-1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#Una vez decidido el hiperparámetro, entreno con todos los datos\n",
    "sentiment_model = LogisticRegression(C=1,max_iter=1500,n_jobs=-1)\n",
    "sentiment_model.fit(embeddings, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4ba3ecb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7717740282806203\n",
      "0.8012\n"
     ]
    }
   ],
   "source": [
    "#Esta precisión no es fiable porque son datos utilizados para el entreno pero vemos que la precisión no ha mejorado mucho\n",
    "#respecto a antes así que no parece haber overfitting\n",
    "print(accuracy_score(y_test, sentiment_model.predict(x_test)))\n",
    "print(accuracy_score(IMDB_Dataset_labels_tmp[-indices], sentiment_model.predict(IMDB_Dataset_embeddings_tmp[-indices,:])))\n",
    "#Todo el resto de pelis no vistas las clasifica bien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "67cd3bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad4bb3cfa42e402b90293da153f18946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([-1,  1,  0, -1,  0,  0,  1, -1,  0], dtype=int64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Algunas pruebas rápidas\n",
    "model = SentenceTransformer(\"paraphrase-multilingual-mpnet-base-v2\")\n",
    "prueba=np.array(model.encode(list(['Estoy hasta las narices, no puedo más',\n",
    "                                   'Me ha salido un día bastante bueno, a seguir trabajando',\n",
    "                                   'El otro día me encontré un erizo',\n",
    "                                   'El otro día me encontré un erizo y me mordió',\n",
    "                                  'El otro día me encontré un erizo y me mordió. Sin embargo, no me quejo, estoy bien con mi vida',\n",
    "                                   'Im quite a bit nervous',\n",
    "                                   'Best day of my life',\n",
    "                                   'How dare you to do this to me?',\n",
    "                                  'El otro día fui al cine después de tanto tiempo... La peli estuvo bien, sin más.']), \n",
    "                                            show_progress_bar=True,batch_size=128))\n",
    "sentiment_model.predict(prueba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6e121fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardo el modelo\n",
    "pickle.dump(sentiment_model, open('sentiment_model.sav', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
